{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv # Used for computing the inverse of matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Use `pip install matplotlib` in command line if matplotlib is not installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "data = np.loadtxt(open('sat_gpa.csv'), delimiter=',')\n",
    "print('shape of original data:', data.shape) # Check if data is 105 by 3\n",
    "\n",
    "# Normalize data\n",
    "data_norm = data / data.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1\n",
    "\n",
    "\n",
    "Implement the Normal Equation method for linear regression: $\\theta = (X^T X)^{-1}X^T y$\n",
    "\n",
    "Use the learned $\\theta$ to make predictions: $\\hat{y} = X\\theta$\n",
    "\n",
    "Compute the residual sum of squares of the model: $RSS = \\sum_i (\\hat{y}^{(i)} - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix X and y\n",
    "# X has three columns: \n",
    "#   - The first column contain all 1s, which is for the intercept\n",
    "#   - The second and third columns contain features, i.e., the 1st and 2nd columns of data_norm\n",
    "# y has one column, i.e., the 3rd column of data_norm\n",
    "\n",
    "X = np.ones_like(data_norm)\n",
    "#### START YOUR CODE ####\n",
    "X[:, 1:3] = data_norm[:,[0,1]]\n",
    "y = data_norm[:,2]\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "\n",
    "# Compute theta using normal equation method\n",
    "# Hint: use the inv() function imported from numpy.linalg\n",
    "#### START YOUR CODE ####\n",
    "theta_method1 = inv(np.transpose(X).dot(X)).dot(np.transpose(X).dot(y))\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "\n",
    "# Use the theta obtained to make predictions and compute the residuals\n",
    "# Hint: use numpy.dot() and numpy.sum(), and avoid using for loops\n",
    "#### START YOUR CODE ####\n",
    "y_hat = np.dot(X, theta_method1)\n",
    "RSS1 = np.sum(np.square(np.dot(X, theta_method1) - y))\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "# Compute residuals\n",
    "\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "print('Theta obtained from normal equation:', theta_method1)\n",
    "print('Residual sum of squares (RSS): ', RSS1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected ouput\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "Theta obtained from normal equation: | [-0.06234478  0.62017319  0.43647674]\n",
    "Residual sum of squares (RSS): | 0.7590471383029533\n",
    "\n",
    "---\n",
    "\n",
    "## Task 2\n",
    "\n",
    "\n",
    "Implement the Gradient Descent method for linear regression.\n",
    "\n",
    "The cost function: $J(\\theta_0, \\theta_1, \\theta_2) = \\frac{1}{2m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2m}\\sum_i (\\theta_0 + \\theta_1 x_1^{(i)} + \\theta_2 x_2^{(i)} - y^{(i)})^2$\n",
    "\n",
    "Gradients w.r.t. parameters: $\\frac{\\partial J}{\\partial \\theta} = \\begin{cases}\\frac{\\partial J}{\\partial \\theta_0}\\\\ \\frac{\\partial J}{\\partial \\theta_1}\\\\ \\frac{\\partial J}{\\partial \\theta_2}\\\\ \\end{cases} = \\begin{cases}\\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})\\\\ \\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})x_1^{(i)}\\\\ \\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})x_2^{(i)}\\\\\\end{cases}$\n",
    "\n",
    "The formula to update parameters at each iteration: $\\theta := \\theta - \\alpha * \\frac{\\partial J}{\\partial \\theta}$\n",
    "\n",
    "Note that $X$, $y$, and $\\theta$ are all vectors (numpy arrays), and thus the operations above should be implemented in a vectorized fashion. Use `numpy.sum()`, `numpy.dot()` and other vectorized functions, and avoid writing `for` loops in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradientDescent function\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Params\n",
    "        X - Shape: (m,3); m is the number of data examples\n",
    "        y - Shape: (m,)\n",
    "        theta - Shape: (3,)\n",
    "        num_iters - Maximum number of iterations\n",
    "    Return\n",
    "        A tuple: (theta, RSS, cost_array)\n",
    "        theta - the learned model parameters\n",
    "        RSS - residual sum of squares\n",
    "        cost_array - stores the cost value of each iteration. Its shape is (num_iters,)\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_array =[]\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "        #### START YOUR CODE ####\n",
    "        # Make predictions\n",
    "        # Shape of y_hat: m by 1\n",
    "        y_hat = X.dot(theta)\n",
    "        \n",
    "        # Compute the difference between prediction (y_hat) and ground truth label (y)\n",
    "        diff = y_hat-y\n",
    "\n",
    "        # Compute the cost\n",
    "        # Hint: Use the diff computed above\n",
    "        cost = np.sum(np.square(diff))/(2*m)\n",
    "        cost_array.append(cost)\n",
    "\n",
    "        # Compute gradients\n",
    "        # Hint: Use the diff computed above\n",
    "        # Hint: Shape of gradients is the same as theta\n",
    "        gradients =  np.dot(X.transpose(),diff)/m\n",
    "\n",
    "        # Update theta\n",
    "        theta = theta-alpha*gradients\n",
    "        #### END YOUR CODE ####\n",
    "    \n",
    "    # Compute residuals\n",
    "    # Hint: Should use the same code as Task 1\n",
    "    #### START YOUR CODE ####\n",
    "    y_hat = np.dot(X, theta)\n",
    "    RSS = np.sum(np.square(y_hat - y))\n",
    "    #### END YOUR CODE ####\n",
    "\n",
    "    return theta, RSS, cost_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to evaluate the gradientDescent function implemented above\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Define learning rate and maximum iteration number\n",
    "ALPHA = 0.05\n",
    "MAX_ITER = 500\n",
    "\n",
    "# Initialize theta to [0,0,0]\n",
    "theta = np.zeros(3)\n",
    "theta_method2, RSS2, cost_array = gradientDescent(X, y, theta, ALPHA, MAX_ITER)\n",
    "\n",
    "print('Theta obtained from gradient descent:', theta_method2)\n",
    "print('Residual sum of squares (RSS): ', RSS2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "Theta obtained from gradient descent: | [0.29911574 0.32224209 0.31267172]\n",
    "Residual sum of squares (RSS): | 0.8641600584370602\n",
    "\n",
    "\n",
    "**NOTE**: It seems that the model trained with gradient descent has larger RSS than the one obtained with normal equation method.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 3\n",
    "**1 point**\n",
    "\n",
    "Plot the cost against iteration number. This is a common method of examining the performance of gradient descent.\n",
    "\n",
    "Try different values of learning rate, for example, $\\alpha=\\{0.01, 0.005, 0.001\\}$, and see how the cost curves change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START YOUR CODE ####\n",
    "alpha = 0.01\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "theta = np.zeros(3)\n",
    "_, _, cost_array = gradientDescent(X, y, theta, alpha, MAX_ITER)\n",
    "\n",
    "plt.plot(range(0,len(cost_array)), cost_array);\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.title('alpha = {}'.format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START YOUR CODE ####\n",
    "alpha = 0.005\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "theta = np.zeros(3)\n",
    "_, _, cost_array = gradientDescent(X, y, theta, alpha, MAX_ITER)\n",
    "\n",
    "plt.plot(range(0,len(cost_array)), cost_array);\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.title('alpha = {}'.format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START YOUR CODE ####\n",
    "alpha = 0.001\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "theta = np.zeros(3)\n",
    "_, _, cost_array = gradientDescent(X, y, theta, alpha, MAX_ITER)\n",
    "\n",
    "plt.plot(range(0,len(cost_array)), cost_array);\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.title('alpha = {}'.format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
